{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_id(keywords):\n",
    "    text = ''.join(keywords)\n",
    "    text= text.replace(' ','').strip()\n",
    "\n",
    "    word_to_id ={}\n",
    "    for idx,i in enumerate(set(text)):\n",
    "        word_to_id[i] = idx + 1    \n",
    "    \n",
    "    id_to_word = dict((j,i) for i,j in word_to_id.items())    \n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('search_log.csv')\n",
    "df['keyword']=df['keyword'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트로 만들기\n",
    "keywords = df['keyword'].str.lower().tolist()\n",
    "\n",
    "#한글자 짜리 제거\n",
    "keywords = [i for i in keywords if len(i)!=1]\n",
    "keywords_end = [i+'$' for i in keywords]\n",
    "\n",
    "# 딕셔너리 제작\n",
    "word_to_id, id_to_word = make_word_id(keywords_end)\n",
    "\n",
    "# 인코딩\n",
    "keywords_encoded = [np.array([word_to_id[i] for i in word.replace(' ','').strip()]) for word in keywords_end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shft 하여 X, y 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ts =[], []\n",
    "for word in keywords_encoded:\n",
    "    for i in range(len(word)-1):\n",
    "        xs.append(word[:i+1])\n",
    "        ts.append(word[i+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=5\n",
    "vocab_size = len(word_to_id)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩\n",
    "xs_pad = pad_sequences(xs, maxlen=maxlen, padding='post')\n",
    "ts_pad = pad_sequences(ts, maxlen=maxlen, padding='post')\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs_pad, ts_pad, test_size=.9, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(X_train, y_train, vocab_size, batch_size=32):    \n",
    "    while True:        \n",
    "        idx = np.random.choice(np.arange(len(X_train)), size=batch_size, replace=False)\n",
    "        yield X_train[idx], to_categorical(y_train[idx], num_classes=vocab_size)        \n",
    "        \n",
    "def build_callbacks():\n",
    "    checkpointer = ModelCheckpoint(filepath='unet.h5', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "    callbacks = [checkpointer]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 128)            268416    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 5, 512)            788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 2097)           1075761   \n",
      "=================================================================\n",
      "Total params: 2,132,657\n",
      "Trainable params: 2,132,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=maxlen, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(vocab_size, activation=('softmax'))))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbop02\\Anaconda3\\envs\\tbop02\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3082/3082 [==============================] - 40s 13ms/step - loss: 1.2137 - accuracy: 0.5439\n",
      "Epoch 2/20\n",
      "  13/3082 [..............................] - ETA: 39s - loss: 0.9654 - accuracy: 0.6130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbop02\\Anaconda3\\envs\\tbop02\\lib\\site-packages\\keras\\callbacks\\callbacks.py:707: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.8457 - accuracy: 0.6520\n",
      "Epoch 3/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.7378 - accuracy: 0.6808\n",
      "Epoch 4/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.6706 - accuracy: 0.6983\n",
      "Epoch 5/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.6389 - accuracy: 0.7049\n",
      "Epoch 6/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.6047 - accuracy: 0.7153\n",
      "Epoch 7/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.5794 - accuracy: 0.7223\n",
      "Epoch 8/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.5591 - accuracy: 0.7263\n",
      "Epoch 9/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.5415 - accuracy: 0.7323\n",
      "Epoch 10/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.5210 - accuracy: 0.7377\n",
      "Epoch 11/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.5136 - accuracy: 0.7403\n",
      "Epoch 12/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4969 - accuracy: 0.7438\n",
      "Epoch 13/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4854 - accuracy: 0.7496\n",
      "Epoch 14/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4748 - accuracy: 0.7517\n",
      "Epoch 15/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4708 - accuracy: 0.7520\n",
      "Epoch 16/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4629 - accuracy: 0.7564\n",
      "Epoch 17/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4506 - accuracy: 0.7590\n",
      "Epoch 18/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4431 - accuracy: 0.7600\n",
      "Epoch 19/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4396 - accuracy: 0.7613\n",
      "Epoch 20/20\n",
      "3082/3082 [==============================] - 39s 13ms/step - loss: 0.4377 - accuracy: 0.7617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x295c03f14a8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_steps = len(X_train) //batch_size\n",
    "test_steps = len(y_train) //batch_size\n",
    "model.fit_generator(train_generator(X_train, y_train, vocab_size), epochs=20, steps_per_epoch = train_steps, callbacks = build_callbacks())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, word, word_to_id, id_to_word):      \n",
    "    \n",
    "    while True:\n",
    "        X_input = [word_to_id[i] for i in word]     \n",
    "        X_input = pad_sequences([X_input], maxlen=maxlen, padding='post')\n",
    "        score = model.predict(X_input)\n",
    "        idx = np.argmax(score[0, 0 ,:])\n",
    "        pred_word = id_to_word[idx]\n",
    "        \n",
    "        if pred_word =='$' or len(word) == maxlen:\n",
    "            break\n",
    "            \n",
    "        word += pred_word\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레몬\n",
      "라디치오\n",
      "토마토\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, '레', word_to_id, id_to_word))\n",
    "print(generate(model, '라', word_to_id, id_to_word))\n",
    "print(generate(model, '토', word_to_id, id_to_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tbop02",
   "language": "python",
   "name": "tbop02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
